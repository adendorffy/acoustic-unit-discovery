{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import segment\n",
    "def Kmeans():\n",
    "    model = KMeans(100)\n",
    "    checkpoint = torch.hub.load_state_dict_from_url(\n",
    "    \"https://github.com/bshall/dusted/releases/download/v0.1/kmeans-english-50f36a.pt\"\n",
    "    )\n",
    "    model.__dict__[\"n_features_in_\"] = checkpoint[\"n_features_in_\"]\n",
    "    model.__dict__[\"_n_threads\"] = checkpoint[\"_n_threads\"]\n",
    "    model.__dict__[\"cluster_centers_\"] = checkpoint[\"cluster_centers_\"].numpy()\n",
    "    return model, segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_file(paths, codebook, segment, gamma):\n",
    "    in_path, out_path = paths\n",
    "    sequence = np.load(in_path)\n",
    "    codes, boundaries = segment(sequence, codebook, gamma)\n",
    "    np.savez(out_path.with_suffix(\".npz\"), codes=codes, boundaries=boundaries)\n",
    "    return sequence.shape[0], np.mean(np.diff(boundaries))\n",
    "\n",
    "def segment_dataset(args):\n",
    "    kmeans, segment = Kmeans()\n",
    "    in_paths = list(args.in_dir.rglob(\"*.npy\"))\n",
    "    out_paths = [args.out_dir / path.relative_to(args.in_dir) for path in in_paths]\n",
    "\n",
    "    for path in out_paths:\n",
    "        path.parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    results = []\n",
    "    for path in tqdm(zip(in_paths, out_paths), desc=\"Processing segments\"):\n",
    "        result = process_file(paths=path, codebook=kmeans.cluster_centers_, segment=segment.segment, gamma=1)\n",
    "        \n",
    "        results.append(result)\n",
    "\n",
    "    frames, boundary_length = zip(*results)\n",
    "    print(f\"Segmented {sum(frames) * 0.02 / 60 / 60:.2f} hours of audio\")\n",
    "    print(f\"Average segment length: {np.mean(boundary_length) * 0.02:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2703/2703 [00:00<00:00, 87306.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented 5.38 hours of audio\n",
      "Average segment length: 1.83 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Args:\n",
    "    def __init__(self, in_dir, out_dir):\n",
    "        self.in_dir =in_dir\n",
    "        self.out_dir = out_dir\n",
    "\n",
    "args = Args(Path(\"features/\"),Path(\"codes/\"))\n",
    "\n",
    "segment_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_bounds = [0, 14, 25, 30, 35, 36]\n",
    "boundaries = [0, 13, 25, 27, 29, 30, 31, 32, 36]\n",
    "encodings = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "\n",
    "features = []\n",
    "prev_j = 1\n",
    "for i in range(1, len(true_bounds)):\n",
    "    new_feature = []\n",
    "    for j in range(prev_j, len(boundaries)):\n",
    "        if true_bounds[i] < boundaries[j]:\n",
    "            continue\n",
    "\n",
    "        if true_bounds[i] == boundaries[j]:\n",
    "            new_feature.append(encodings[j-1])\n",
    "            \n",
    "        elif true_bounds[i] > boundaries[j]:\n",
    "            new_feature.append(encodings[j-1])\n",
    "        \n",
    "        prev_j = j + 1\n",
    "        \n",
    "    features.append(new_feature)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg size cluster for file output/librispeech_subset/wavlm_base/8/0.05/norm_distance_matrix.npy: 1.9774436090225564\n",
      "Avg size cluster for file output/librispeech_subset/wavlm_base/8/0.1/norm_distance_matrix.npy: 2.8452380952380953\n",
      "Avg size cluster for file output/librispeech_subset/wavlm_base/8/0.15/norm_distance_matrix.npy: 4.326923076923077\n",
      "Avg size cluster for file output/librispeech_subset/wavlm_base/8/0.2/norm_distance_matrix.npy: 5.666666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "in_dir = Path(\"output/librispeech_subset/\")\n",
    "\n",
    "for path in list(in_dir.rglob(\"*.npy\")):\n",
    "    mat = np.load(path)\n",
    "\n",
    "    DISTANCE_THRESHOLD = round(np.mean(mat)/3,3)\n",
    "    num_nodes = mat.shape[0]\n",
    "    graph = {i: set() for i in range(num_nodes)}\n",
    "\n",
    "    for i in range(num_nodes - 1): \n",
    "        for j in range(i + 1, num_nodes):  \n",
    "            if mat[i, j] < DISTANCE_THRESHOLD:\n",
    "                graph[i].add(j)\n",
    "                graph[j].add(i)  \n",
    "\n",
    "\n",
    "    clusters = []\n",
    "    visited = set()\n",
    "\n",
    "    def bfs(start_node):\n",
    "        \"\"\" Traverse a cluster using BFS \"\"\"\n",
    "        queue = [start_node]\n",
    "        cluster = []\n",
    "        \n",
    "        while queue:\n",
    "            node = queue.pop(0)\n",
    "            if node in visited:\n",
    "                continue\n",
    "            visited.add(node)\n",
    "            cluster.append(node)\n",
    "            queue.extend(graph[node])  \n",
    "\n",
    "        return cluster\n",
    "\n",
    "    for node in range(num_nodes):\n",
    "        if node not in visited:\n",
    "            new_cluster = bfs(node)\n",
    "            clusters.append(new_cluster)\n",
    "\n",
    "    sizes = []\n",
    "    for i, clust in enumerate(clusters):\n",
    "        sizes.append(len(clust))\n",
    "\n",
    "    print(f\"Avg size cluster for file {str(path)}: {np.mean(sizes)}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
