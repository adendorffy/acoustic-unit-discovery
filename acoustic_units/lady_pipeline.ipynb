{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample files to read\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "in_dir = Path(\"data/ladys\")\n",
    "sample_size = 20\n",
    "\n",
    "wav_paths = list(in_dir.rglob(\"*.wav\"))\n",
    "if sample_size < len(wav_paths):\n",
    "    sampled_paths = random.sample(wav_paths, sample_size)\n",
    "else:\n",
    "    sampled_paths = wav_paths  \n",
    "\n",
    "print(len(sampled_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/danel/.cache/torch/hub/bshall_hubert_main\n",
      "Encoding Audio Features: 100%|██████████| 6/6 [00:00<00:00, 29.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchaudio.functional import resample\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "hubert = torch.hub.load(\n",
    "    \"bshall/hubert:main\",\n",
    "    f\"hubert_discrete\",\n",
    "    trust_repo=True,\n",
    ")\n",
    "\n",
    "acoustic_units = {}\n",
    "filenames = {}\n",
    "index = 0\n",
    "for wav_path in tqdm(sampled_paths, desc=\"Encoding Audio Features\"):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    wav = resample(wav, sr, 16000)\n",
    "    wav = wav.unsqueeze(0)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        units = hubert.units(wav)\n",
    "    acoustic_units[wav_path.stem] = units.numpy()\n",
    "    filenames[index] = wav_path.stem\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white6\n",
      "[59 35 43 50 50 51 48 20 20 20 78 78]\n",
      "\n",
      "rose5\n",
      "[97 24 13 13 13  8  8  8 85 85 85 75 73 73 29 92 30 34]\n",
      "\n",
      "lady16\n",
      "[17 17 17 95 48 48 48 48 48 20 82 82 40 57 57 57 91 91 91  5 33 78 78 74\n",
      " 74]\n",
      "\n",
      "lady2\n",
      "[ 6  6 90 17 17 17 17 95 48 48 48 48 48 20 20 82 82 40 40 57 57 91 91 91\n",
      "  5  5 16 74 74]\n",
      "\n",
      "lady7\n",
      "[17 17 95 95 48 48 48 48 20 82 82 82 40 57 57 57 91 91 33 33 78 78 74 74]\n",
      "\n",
      "lady0\n",
      "[ 6  6 90 17 17 17 17 95 95 48 48 48 48 91 20 33 63 82 40 40 91 91 91 91\n",
      " 91 91  5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in acoustic_units:\n",
    "    print(path)\n",
    "    print(acoustic_units[path])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_model(url):\n",
    "    model = KMeans(100)\n",
    "    checkpoint = torch.hub.load_state_dict_from_url(url)\n",
    "\n",
    "    model.__dict__[\"n_features_in_\"] = checkpoint[\"n_features_in_\"]\n",
    "    model.__dict__[\"_n_threads\"] = checkpoint[\"_n_threads\"]\n",
    "    model.__dict__[\"cluster_centers_\"] = checkpoint[\"cluster_centers_\"].numpy()\n",
    "    return model\n",
    "\n",
    "\n",
    "kmeans_url = \"https://github.com/bshall/dusted/releases/download/v0.1/kmeans-english-50f36a.pt\"\n",
    "kmeans = kmeans_model(kmeans_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Audio Features: 100%|██████████| 6/6 [00:00<00:00, 21.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from segment import segment\n",
    "\n",
    "layer = 8\n",
    "gamma = 0.05\n",
    "\n",
    "bundle = torchaudio.pipelines.WAVLM_BASE\n",
    "model = bundle.get_model()\n",
    "model.eval()\n",
    "\n",
    "encodings = {}\n",
    "dusted_units = {}\n",
    "for wav_path in tqdm(sampled_paths, desc=\"Encoding Audio Features\"):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        encoding, _ = model.extract_features(wav, num_layers=layer)\n",
    "\n",
    "    encoding = encoding[layer-1].squeeze().cpu().numpy()\n",
    "    encodings[wav_path.stem] = encoding\n",
    "\n",
    "    codes, _ = segment(encoding, kmeans.cluster_centers_, gamma)   \n",
    "    dusted_units[wav_path.stem] = codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white6\n",
      "[95 53 97 95 97]\n",
      "[59 35 43 50 50 51 48 20 20 20 78 78]\n",
      "\n",
      "rose5\n",
      "[97 95 18 97]\n",
      "[97 24 13 13 13  8  8  8 85 85 85 75 73 73 29 92 30 34]\n",
      "\n",
      "lady16\n",
      "[18 97 18 97 18 95 97 95]\n",
      "[17 17 17 95 48 48 48 48 48 20 82 82 40 57 57 57 91 91 91  5 33 78 78 74\n",
      " 74]\n",
      "\n",
      "lady2\n",
      "[95 97 95 18 95]\n",
      "[ 6  6 90 17 17 17 17 95 48 48 48 48 48 20 20 82 82 40 40 57 57 91 91 91\n",
      "  5  5 16 74 74]\n",
      "\n",
      "lady7\n",
      "[97 24 95 18 97 95]\n",
      "[17 17 95 95 48 48 48 48 20 82 82 82 40 57 57 57 91 91 33 33 78 78 74 74]\n",
      "\n",
      "lady0\n",
      "[95 18 97 24 95 18 97 95]\n",
      "[ 6  6 90 17 17 17 17 95 95 48 48 48 48 91 20 33 63 82 40 40 91 91 91 91\n",
      " 91 91  5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_units = []\n",
    "a_units = []\n",
    "for path in dusted_units:\n",
    "    print(path)\n",
    "    print(dusted_units[path])\n",
    "    d_units.append(dusted_units[path])\n",
    "    print(acoustic_units[path])\n",
    "    a_units.append(acoustic_units[path])\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. 18. 21. 26. 20. 25.]\n",
      " [ 0.  0. 25. 29. 24. 27.]\n",
      " [ 0.  0.  0.  9.  5. 17.]\n",
      " [ 0.  0.  0.  0. 12. 10.]\n",
      " [ 0.  0.  0.  0.  0. 17.]\n",
      " [ 0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import editdistance\n",
    "import numpy as np\n",
    "\n",
    "num_features = len(a_units)\n",
    "dist_mat = np.zeros((num_features, num_features))\n",
    "\n",
    "for u in range(num_features):\n",
    "    for v in range(u + 1, num_features):\n",
    "        dist_mat[u, v] = editdistance.eval(a_units[u], a_units[v])\n",
    "\n",
    "print(dist_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(dist_mat, distance_threshold):\n",
    "    num_nodes = dist_mat.shape[0]\n",
    "    graph = {i: set() for i in range(num_nodes)}\n",
    "\n",
    "    for i in range(num_nodes - 1): \n",
    "        for j in range(i + 1, num_nodes):  \n",
    "            if dist_mat[i, j] < distance_threshold:\n",
    "                graph[i].add(j)\n",
    "                graph[j].add(i)  \n",
    "\n",
    "\n",
    "    clusters = []\n",
    "    visited = set()\n",
    "\n",
    "    def bfs(start_node):\n",
    "        \"\"\" Traverse a cluster using BFS \"\"\"\n",
    "        queue = [start_node]\n",
    "        cluster = []\n",
    "        \n",
    "        while queue:\n",
    "            node = queue.pop(0)\n",
    "            if node in visited:\n",
    "                continue \n",
    "            visited.add(node)\n",
    "            cluster.append(node)\n",
    "            queue.extend(graph[node])  \n",
    "\n",
    "        return cluster\n",
    "\n",
    "    for node in range(num_nodes):\n",
    "        if node not in visited:\n",
    "            new_cluster = bfs(node)\n",
    "            clusters.append(new_cluster)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_mat += dist_mat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "au_clusters = cluster(dist_mat, 15)\n",
    "print(len(au_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_to_dict(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    data_dict = {}\n",
    "    current_id = None\n",
    "    word_dict = {}\n",
    "\n",
    "    for line in lines: \n",
    "        line = line.strip()\n",
    "\n",
    "        if not line: \n",
    "            continue\n",
    "        \n",
    "        if line.endswith(\":\") and not line.split(\":\")[0].isdigit():\n",
    "            if current_id is not None:\n",
    "                data_dict[current_id] = word_dict\n",
    "            \n",
    "            current_id = line[:-1]\n",
    "            word_dict = {}\n",
    "        else:\n",
    "            parts = line.split(\": \")\n",
    "            if len(parts) == 2:\n",
    "                index, word = parts\n",
    "                word_dict[int(index)] = word.strip()\n",
    "            else:\n",
    "                parts = parts[0].split(\":\")\n",
    "                index = parts[0]\n",
    "                word_dict[int(index)] = \" \"\n",
    "            \n",
    "            if current_id is not None:\n",
    "                data_dict[current_id] = word_dict\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "true_words_dict = parse_text_to_dict(\"data/words_and_indices.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster and WordUnit classes\n",
    "from collections import defaultdict\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self,id, word_dict=None, true_words=None):\n",
    "        self.id = id\n",
    "        self.length = len(word_dict) if word_dict else 0\n",
    "        self.word_dict = word_dict if word_dict is not None else []\n",
    "        self.true_word_dict = true_words if true_words is not None else []\n",
    "    \n",
    "    def add_word_unit(self, id, index, file):\n",
    "        word_unit = WordUnit(file, index, id)\n",
    "        self.length += 1\n",
    "        self.word_dict.append(word_unit)\n",
    "\n",
    "    def add_true_word(self, word):\n",
    "        self.true_word_dict.append(word)\n",
    "\n",
    "    @classmethod\n",
    "    def print_cluster(self, cluster):\n",
    "        print(f\"Cluster {cluster.id}\")\n",
    "        for word in cluster.word_dict:\n",
    "            print(f\"Word {word.id}: Index {word.index} in File {word.file}\")\n",
    "    \n",
    "    def cluster_purity(self):\n",
    "\n",
    "        word_counts = {}\n",
    "        for word in self.true_word_dict:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "        max_count = max(word_counts.values()) if word_counts else 0\n",
    "        cluster_purity = max_count / self.length if self.length > 0 else 0\n",
    "\n",
    "        self.purity = cluster_purity\n",
    "\n",
    "    @classmethod\n",
    "    def duplicate_clusters(self, clusters):\n",
    "        cluster_dict = defaultdict(int)\n",
    "\n",
    "        for cluster in clusters:\n",
    "            cluster_set = frozenset(cluster)  \n",
    "            cluster_dict[cluster_set] += 1  \n",
    "\n",
    "        duplicate_count = sum(1 for count in cluster_dict.values() if count > 1)\n",
    "\n",
    "        return duplicate_count\n",
    "\n",
    "class WordUnit:\n",
    "    def __init__(self, file, index, id):\n",
    "        self.index = int(index)\n",
    "        self.file = file\n",
    "        self.id = int(id)\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "\n",
    "    def add_word_boundaries(self, start_time, end_time):\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "white6\n",
      "\n",
      "Cluster 1\n",
      "rose5\n",
      "\n",
      "Cluster 2\n",
      "lady16\n",
      "lady2\n",
      "lady7\n",
      "lady0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, clust in enumerate(au_clusters):\n",
    "    print(f\"Cluster {i}\")\n",
    "    for w in range(len(clust)):\n",
    "        if clust[w] in filenames:\n",
    "            filename = filenames[clust[w]]\n",
    "            print(filename)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
