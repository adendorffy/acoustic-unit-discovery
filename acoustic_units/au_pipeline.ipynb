{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample files to read\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "in_dir = Path(\"data/dev-clean\")\n",
    "sample_size = 20\n",
    "\n",
    "wav_paths = list(in_dir.rglob(\"*.flac\"))\n",
    "wav_paths = [\n",
    "    Path(\"data/dev-clean/174/50561/174-50561-0005.flac\"),\n",
    "    Path(\"data/dev-clean/174/50561/174-50561-0013.flac\"),\n",
    "    Path(\"data/dev-clean/3081/166546/3081-166546-0003.flac\"),\n",
    "             ]\n",
    "wav_paths = [Path(\"data/dev-clean/3000/15664/3000-15664-0017.flac\")]\n",
    "\n",
    "if sample_size < len(wav_paths):\n",
    "    sampled_paths = random.sample(wav_paths, sample_size)\n",
    "else:\n",
    "    sampled_paths = wav_paths  \n",
    "\n",
    "print(len(sampled_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/daneladendorff/.cache/torch/hub/bshall_hubert_main\n",
      "Encoding Audio Features: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchaudio.functional import resample\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "hubert = torch.hub.load(\n",
    "    \"bshall/hubert:main\",\n",
    "    \"hubert_discrete\",\n",
    "    trust_repo=True,\n",
    ")\n",
    "\n",
    "acoustic_units = {}\n",
    "for wav_path in tqdm(sampled_paths, desc=\"Encoding Audio Features\"):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    wav = resample(wav, sr, 16000)\n",
    "    wav = wav.unsqueeze(0)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        units = hubert.units(wav)\n",
    "    acoustic_units[wav_path.stem] = units.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/daneladendorff/.cache/torch/hub/bshall_dusted_main\n",
      "Using cache found in /Users/daneladendorff/.cache/torch/hub/bshall_hubert_main\n",
      "Encoding Audio Features: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer = 7\n",
    "hubert, encode = torch.hub.load(\n",
    "    \"bshall/dusted:main\", \"hubert\", language=\"english\", trust_repo=True\n",
    ")\n",
    "\n",
    "encodings = {}\n",
    "for wav_path in tqdm(sampled_paths, desc=\"Encoding Audio Features\"):\n",
    "    wav, sr = torchaudio.load(wav_path)\n",
    "    wav = resample(wav, sr, 16000)\n",
    "    wav = wav.unsqueeze(0)\n",
    "    x = encode(hubert, wav, layer)\n",
    "    encodings[wav_path.stem] = x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cutting Encodings: 100%|██████████| 1/1 [00:00<00:00, 483.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n",
      "torch.Size([0, 704, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "align_dir = Path(\"data/all_alignments\")\n",
    "align_paths = list(align_dir.rglob(\"*.list\"))\n",
    "\n",
    "def get_frame_num(timestamp, sample_rate, frame_size_ms):\n",
    "    hop = frame_size_ms/1000 * sample_rate\n",
    "    hop_size = np.max([hop, 1])\n",
    "    return int((timestamp * sample_rate) / hop_size)\n",
    "\n",
    "filenames = {}\n",
    "cut_encodings = {}\n",
    "index = 1\n",
    "for path in tqdm(encodings, desc=\"Cutting Encodings\"):\n",
    "    alignment_file = [a for a in align_paths if a.stem == path]\n",
    "    if not alignment_file:\n",
    "        continue\n",
    "    else:\n",
    "        alignment_file = alignment_file[0]\n",
    "\n",
    "    with open(str(alignment_file), \"r\") as f:\n",
    "        bounds = [get_frame_num(float(line.strip()), 16000, 20) for line in f]\n",
    "    \n",
    "    cut_x = encodings[path][0:bounds[0]]\n",
    "    cuttings = [cut_x]\n",
    "    filenames[0] = f\"{path}_{0}\"\n",
    "    \n",
    "    for i in range(len(bounds)-1): \n",
    "        cut_x = encodings[path][bounds[i]: bounds[i+1]]\n",
    "        print(cut_x.shape)\n",
    "        cuttings.append(cut_x)\n",
    "        filenames[index] = f\"{path}_{i+1}\"\n",
    "        index += 1\n",
    "    cut_encodings[path] = cuttings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/daneladendorff/.cache/torch/hub/bshall_dusted_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0857,  0.0068, -0.0018,  ..., -0.1621,  0.0876,  0.0083],\n",
      "        [-0.0787,  0.0457, -0.0113,  ..., -0.1585,  0.0800, -0.0450],\n",
      "        [-0.0756,  0.0604, -0.0455,  ..., -0.1574,  0.0271, -0.1214],\n",
      "        ...,\n",
      "        [ 0.0888,  0.1911, -0.0035,  ..., -0.0237,  0.0802,  0.0904],\n",
      "        [-0.1122,  0.0095,  0.0040,  ..., -0.0382,  0.1739,  0.2278],\n",
      "        [-0.1573, -0.0210,  0.0655,  ..., -0.0874,  0.1755,  0.1598]])\n",
      "tensor([], size=(0, 704, 768))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "XA must be a 2-dimensional array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(sequence)\n\u001b[0;32m---> 13\u001b[0m     codes, boundaries \u001b[38;5;241m=\u001b[39m \u001b[43msegment\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_centers_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     words\u001b[38;5;241m.\u001b[39mappend(codes)\n\u001b[1;32m     15\u001b[0m dusted_units\u001b[38;5;241m.\u001b[39mappend(words)\n",
      "File \u001b[0;32m~/.cache/torch/hub/bshall_dusted_main/dusted/segment.py:22\u001b[0m, in \u001b[0;36msegment\u001b[0;34m(sequence, codebook, gamma)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msegment\u001b[39m(\n\u001b[1;32m      9\u001b[0m     sequence: np\u001b[38;5;241m.\u001b[39mndarray, codebook: np\u001b[38;5;241m.\u001b[39mndarray, gamma: \u001b[38;5;28mfloat\u001b[39m\n\u001b[1;32m     10\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Group speech representations into phone-like segments.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m        NDArray[int]: list of segment boundaries of shape (N+1,).\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     dists \u001b[38;5;241m=\u001b[39m \u001b[43mdistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodebook\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     23\u001b[0m     alpha, P \u001b[38;5;241m=\u001b[39m _segment(dists, gamma)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _backtrack(alpha, P)\n",
      "File \u001b[0;32m~/Desktop/adendorffy/.env/lib/python3.10/site-packages/scipy/spatial/distance.py:3104\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   3101\u001b[0m sB \u001b[38;5;241m=\u001b[39m XB\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   3103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 3104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXA must be a 2-dimensional array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sB) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXB must be a 2-dimensional array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: XA must be a 2-dimensional array."
     ]
    }
   ],
   "source": [
    "kmeans, segment = torch.hub.load(\n",
    "    \"bshall/dusted:main\", \"kmeans\", language=\"english\", trust_repo=True\n",
    ")\n",
    "\n",
    "gamma = 0.001\n",
    "\n",
    "dusted_units = []\n",
    "for path in cut_encodings:\n",
    "    words = []\n",
    "    for word in cut_encodings[path]:\n",
    "        sequence = word.squeeze(0)\n",
    "        print(sequence)\n",
    "        codes, boundaries = segment(sequence, kmeans.cluster_centers_, gamma)\n",
    "        words.append(codes)\n",
    "    dusted_units.append(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cutting Encodings: 100%|██████████| 1/1 [00:00<00:00, 500.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "align_dir = Path(\"data/all_alignments\")\n",
    "align_paths = list(align_dir.rglob(\"*.list\"))\n",
    "\n",
    "def get_frame_num(timestamp, sample_rate, frame_size_ms):\n",
    "    hop = frame_size_ms/1000 * sample_rate\n",
    "    hop_size = np.max([hop, 1])\n",
    "    return int((timestamp * sample_rate) / hop_size)\n",
    "\n",
    "filenames = {}\n",
    "cut_units = {}\n",
    "index = 1\n",
    "for path in tqdm(acoustic_units, desc=\"Cutting Encodings\"):\n",
    "    alignment_file = [a for a in align_paths if a.stem == path]\n",
    "    if not alignment_file:\n",
    "        continue\n",
    "    else:\n",
    "        alignment_file = alignment_file[0]\n",
    "\n",
    "    with open(str(alignment_file), \"r\") as f:\n",
    "        bounds = [get_frame_num(float(line.strip()), 16000, 20) for line in f]\n",
    "    \n",
    "    cut_unit = acoustic_units[path][0:bounds[0]]\n",
    "    cuttings = [cut_unit]\n",
    "    filenames[0] = f\"{path}_{0}\"\n",
    "    \n",
    "    for i in range(len(bounds)-1): \n",
    "        cut_unit = acoustic_units[path][bounds[i]: bounds[i+1]]\n",
    "        cuttings.append(cut_unit)\n",
    "        filenames[index] = f\"{path}_{i+1}\"\n",
    "        index += 1\n",
    "    cut_units[path] = cuttings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000-15664-0017_0: [ 6  6  6  6  6 44 44 44 96 96 96 96 96 96 96 96 22 22 22]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for w in filenames:\n",
    "    parts = filenames[w].split(\"_\")\n",
    "    name = parts[0]\n",
    "    i = int(parts[1])\n",
    "    print(f\"{filenames[w]}: {cut_units[name][i]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000-15664-0017\n",
      "[ 6  6  6  6  6 44 44 44 96 96 96 96 96 96 96 96 22 22 22]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in cut_units:\n",
    "    print(path)\n",
    "    for word in cut_units[path]:\n",
    "        print(word)\n",
    "        print()\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "just_words = []\n",
    "\n",
    "index = 0\n",
    "for path in cut_units:\n",
    "    for i in range(len(cut_units[path])):\n",
    "        just_words.append(cut_units[path][i])\n",
    "\n",
    "num_words = len(just_words)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def edit_distance(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Compute the edit distance between two sequences using dynamic programming.\n",
    "    \"\"\"\n",
    "    N, M = len(seq1), len(seq2)\n",
    "    dp = np.zeros((N + 1, M + 1))\n",
    "    for i in range(N + 1):\n",
    "        dp[i, 0] = i\n",
    "    for j in range(M + 1):\n",
    "        dp[0, j] = j\n",
    "    for i in range(1, N + 1):\n",
    "        for j in range(1, M + 1):\n",
    "            cost = 0 if seq1[i - 1] == seq2[j - 1] else 1\n",
    "            dp[i, j] = min(dp[i - 1, j] + 1, dp[i, j - 1] + 1, dp[i - 1, j - 1] + cost)\n",
    "    return dp[N, M] \n",
    "\n",
    "def calculate_distance(just_words, num_words):\n",
    "    dist_mat = np.zeros((num_words, num_words))\n",
    "\n",
    "    for i in tqdm(range(num_words), desc=\"Calculating Distances\"):\n",
    "        js = [j for j in range(i + 1, num_words)]\n",
    "        dists_i = Parallel(n_jobs=8)(\n",
    "            delayed(edit_distance)(just_words[i], just_words[j]) for j in js\n",
    "        )\n",
    "\n",
    "        for j, dist in zip(js, dists_i):\n",
    "            dist_mat[i, j] = dist\n",
    "            dist_mat[j, i] = dist  \n",
    "    \n",
    "    return dist_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Distances: 100%|██████████| 39/39 [00:02<00:00, 18.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. 19. 19. 19. 25.]\n",
      " [19.  0. 17.  6. 23.]\n",
      " [19. 17.  0. 16. 25.]\n",
      " [19.  6. 16.  0. 25.]\n",
      " [25. 23. 25. 25.  0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dist_mat = calculate_distance(just_words, num_words)\n",
    "print(dist_mat[0:5, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. 19. 19. 19. 25.]\n",
      " [ 0.  0. 17.  6. 23.]\n",
      " [ 0.  0.  0. 16. 25.]\n",
      " [ 0.  0.  0.  0. 25.]\n",
      " [ 0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import editdistance\n",
    "import numpy as np\n",
    "\n",
    "dist_mat = np.zeros((num_words, num_words))\n",
    "\n",
    "for u in range(num_words):\n",
    "    for v in range(u + 1, num_words):\n",
    "        dist_mat[u, v] = editdistance.eval(just_words[u], just_words[v])\n",
    "\n",
    "print(dist_mat[0:5, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering algorithm \n",
    "def cluster(dist_mat, distance_threshold):\n",
    "    num_nodes = dist_mat.shape[0]\n",
    "    graph = {i: set() for i in range(num_nodes)}\n",
    "\n",
    "    for i in range(num_nodes - 1): \n",
    "        for j in range(i + 1, num_nodes):  \n",
    "            if dist_mat[i, j] < distance_threshold:\n",
    "                graph[i].add(j)\n",
    "                graph[j].add(i)  \n",
    "\n",
    "\n",
    "    clusters = []\n",
    "    visited = set()\n",
    "\n",
    "    def bfs(start_node):\n",
    "        \"\"\" Traverse a cluster using BFS \"\"\"\n",
    "        queue = [start_node]\n",
    "        cluster = []\n",
    "        \n",
    "        while queue:\n",
    "            node = queue.pop(0)\n",
    "            if node in visited:\n",
    "                continue \n",
    "            visited.add(node)\n",
    "            cluster.append(node)\n",
    "            queue.extend(graph[node])  \n",
    "\n",
    "        return cluster\n",
    "\n",
    "    for node in range(num_nodes):\n",
    "        if node not in visited:\n",
    "            new_cluster = bfs(node)\n",
    "            clusters.append(new_cluster)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "au_clusters = cluster(dist_mat, 20)\n",
    "print(len(au_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true word dict\n",
    "def parse_text_to_dict(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    data_dict = {}\n",
    "    current_id = None\n",
    "    word_dict = {}\n",
    "\n",
    "    for line in lines: \n",
    "        line = line.strip()\n",
    "\n",
    "        if not line: \n",
    "            continue\n",
    "        \n",
    "        if line.endswith(\":\") and not line.split(\":\")[0].isdigit():\n",
    "            if current_id is not None:\n",
    "                data_dict[current_id] = word_dict\n",
    "            \n",
    "            current_id = line[:-1]\n",
    "            word_dict = {}\n",
    "        else:\n",
    "            parts = line.split(\": \")\n",
    "            if len(parts) == 2:\n",
    "                index, word = parts\n",
    "                word_dict[int(index)] = word.strip()\n",
    "            else:\n",
    "                parts = parts[0].split(\":\")\n",
    "                index = parts[0]\n",
    "                word_dict[int(index)] = \" \"\n",
    "            \n",
    "            if current_id is not None:\n",
    "                data_dict[current_id] = word_dict\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "true_words_dict = parse_text_to_dict(\"data/words_and_indices.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster and WordUnit classes\n",
    "from collections import defaultdict\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self,id, word_dict=None, true_words=None):\n",
    "        self.id = id\n",
    "        self.length = len(word_dict) if word_dict else 0\n",
    "        self.word_dict = word_dict if word_dict is not None else []\n",
    "        self.true_word_dict = true_words if true_words is not None else []\n",
    "    \n",
    "    def add_word_unit(self, id, index, file):\n",
    "        word_unit = WordUnit(file, index, id)\n",
    "        self.length += 1\n",
    "        self.word_dict.append(word_unit)\n",
    "\n",
    "    def add_true_word(self, word):\n",
    "        self.true_word_dict.append(word)\n",
    "\n",
    "    @classmethod\n",
    "    def print_cluster(self, cluster):\n",
    "        print(f\"Cluster {cluster.id}\")\n",
    "        for word in cluster.word_dict:\n",
    "            print(f\"Word {word.id}: Index {word.index} in File {word.file}\")\n",
    "    \n",
    "    def cluster_purity(self):\n",
    "\n",
    "        word_counts = {}\n",
    "        for word in self.true_word_dict:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "        max_count = max(word_counts.values()) if word_counts else 0\n",
    "        cluster_purity = max_count / self.length if self.length > 0 else 0\n",
    "\n",
    "        self.purity = cluster_purity\n",
    "\n",
    "    @classmethod\n",
    "    def duplicate_clusters(self, clusters):\n",
    "        cluster_dict = defaultdict(int)\n",
    "\n",
    "        for cluster in clusters:\n",
    "            cluster_set = frozenset(cluster)  \n",
    "            cluster_dict[cluster_set] += 1  \n",
    "\n",
    "        duplicate_count = sum(1 for count in cluster_dict.values() if count > 1)\n",
    "\n",
    "        return duplicate_count\n",
    "\n",
    "class WordUnit:\n",
    "    def __init__(self, file, index, id):\n",
    "        self.index = int(index)\n",
    "        self.file = file\n",
    "        self.id = int(id)\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "\n",
    "    def add_word_boundaries(self, start_time, end_time):\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cluster 1: it,  ,  , feet,  ,  ,  ,  , at, an,  , sea, in, and, five, feet, ice, above, nine, four\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "au_clusters = cluster(dist_mat, 15)\n",
    "clusters = []\n",
    "for i, c in enumerate(au_clusters):\n",
    "    new_c = Cluster(i)\n",
    "    for w in range(len(c)):\n",
    "        if c[w] in filenames:\n",
    "            filename_parts = filenames[c[w]].split(\"_\")\n",
    "            filename = filename_parts[0]\n",
    "            word_index = int(filename_parts[1])        \n",
    "            new_c.add_word_unit(w, word_index, filename)\n",
    "    clusters.append(new_c)\n",
    "\n",
    "for c in clusters:\n",
    "    # print(c.id)\n",
    "    for word_unit in c.word_dict:\n",
    "        # if len(c.word_dict) > 1:\n",
    "        #     print(word_unit.file, word_unit.index)\n",
    "        if word_unit.index == 0:\n",
    "            word = ''\n",
    "        else:\n",
    "            word = true_words_dict[word_unit.file][word_unit.index-1]\n",
    "        \n",
    "        c.add_true_word(word)\n",
    "    print()\n",
    "    \n",
    "    if len(c.word_dict) > 1:  \n",
    "        print(f\"Cluster {c.id}: \", end=\"\")                \n",
    "        print(\", \".join(c.true_word_dict))\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
