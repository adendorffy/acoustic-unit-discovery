{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_name = \"wavlm_base\"\n",
    "layer = 8\n",
    "in_dir = Path(\"data/dev-clean\")\n",
    "sample_size = 10\n",
    "ext = \"flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample files to read\n",
    "import random\n",
    "\n",
    "wav_paths = list(in_dir.rglob(f\"**/*.{ext}\"))\n",
    "sampled_paths = random.sample(wav_paths, sample_size)  \n",
    "\n",
    "print(len(sampled_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Audio Features: 100%|██████████| 10/10 [00:02<00:00,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored Encodings in features/wavlm_base/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode the sampled audio features \n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def preemphasis(signal, coeff=0.97):\n",
    "    return np.append(signal[0], signal[1:] - coeff*signal[:-1])\n",
    "\n",
    "model_pipelines = {\n",
    "    \"hubert_base\": torchaudio.pipelines.HUBERT_BASE,\n",
    "    \"hubert_large\": torchaudio.pipelines.HUBERT_LARGE,\n",
    "    \"hubert_xlarge\": torchaudio.pipelines.HUBERT_XLARGE,\n",
    "    \"wavlm_base\": torchaudio.pipelines.WAVLM_BASE,\n",
    "    \"wavlm_large\": torchaudio.pipelines.WAVLM_LARGE,\n",
    "    \"wavlm_base_plus\": torchaudio.pipelines.WAVLM_BASE_PLUS,\n",
    "}\n",
    "\n",
    "if model_name != \"mfcc\":\n",
    "    bundle = model_pipelines.get(model_name, torchaudio.pipelines.HUBERT_BASE)\n",
    "    model = bundle.get_model()\n",
    "    model.eval()\n",
    "\n",
    "encodings = {}\n",
    "for wav_path in tqdm(sampled_paths, desc=\"Encoding Audio Features\"):\n",
    "    if model_name != \"mfcc\":\n",
    "        out_dir = Path(\"features/\") / model_name / str(layer)\n",
    "        wav, sr = torchaudio.load(wav_path)\n",
    "        wav = torchaudio.functional.resample(wav, sr, 16000)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            encoding, _ = model.extract_features(wav, num_layers=layer)\n",
    "\n",
    "        encoding = encoding[layer-1].squeeze().cpu().numpy()\n",
    "    else:\n",
    "        out_dir = Path(\"features/\") / model_name \n",
    "        wav, sr = librosa.core.load(wav_path, sr=None)\n",
    "        wav = preemphasis(wav, coeff=0.97)\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=wav, sr=sr, n_mfcc=13, n_mels=24, \n",
    "            n_fft=int(np.floor(0.025*sr)),\n",
    "            hop_length=int(np.floor(0.01*sr)), \n",
    "            fmin=64, fmax=8000\n",
    "        )\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        mfcc_delta_delta = librosa.feature.delta(mfcc_delta)\n",
    "        encoding = np.hstack([mfcc.T, mfcc_delta.T, mfcc_delta_delta.T])\n",
    "\n",
    "    if out_dir:\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        output_path = Path(out_dir) / f\"{wav_path.stem}.npy\"\n",
    "        np.save(output_path, encoding)\n",
    "    encodings[wav_path.stem] = encoding\n",
    "print(f\"Stored Encodings in {str(out_dir)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cutting Encodings: 100%|██████████| 10/10 [00:00<00:00, 555.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cut encodings\n",
    "def get_frame_num(timestamp: float, sample_rate: int, frame_size_ms: int)->int:\n",
    "    hop_size = frame_size_ms/1000 * sample_rate\n",
    "    hop_size = np.max([hop_size, 1])\n",
    "    return int((timestamp * sample_rate) / hop_size)\n",
    "\n",
    "out_dir = Path(\"output/codes/kmeans\")\n",
    "align_dir = Path(\"data/all_alignments\")\n",
    "\n",
    "if out_dir and model_name != \"mfcc\":\n",
    "    out_dir = out_dir / model_name / str(layer)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "else:\n",
    "    out_dir = out_dir / model_name \n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "align_paths = list(align_dir.rglob(\"*.list\"))\n",
    "\n",
    "cut_encodings = {}\n",
    "filenames = {}\n",
    "features = []\n",
    "index = 0\n",
    "for path in tqdm(encodings, desc=\"Cutting Encodings\"):\n",
    "    alignment_file = [a for a in align_paths if a.stem == path]\n",
    "    if not alignment_file:\n",
    "        continue\n",
    "    else:\n",
    "        alignment_file = alignment_file[0]\n",
    "\n",
    "    with open(str(alignment_file), \"r\") as f:\n",
    "        bounds = [get_frame_num(float(line.strip()), 16000, 20) for line in f]\n",
    "    \n",
    "    cut_encoding = encodings[path][0: bounds[0]]\n",
    "    words = [cut_encoding]\n",
    "    for i in range(len(bounds)-1): \n",
    "        cut_encoding = encodings[path][bounds[i]: bounds[i+1]]\n",
    "        features.append(cut_encoding)\n",
    "        words.append(cut_encoding)\n",
    "        filenames[index] = f\"{path}_{i}\"\n",
    "        index += 1\n",
    "    cut_encodings[path] = words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dictionaries to arrays and getting the index for each of the words in the dataset\n",
    "dict_ind = {}\n",
    "\n",
    "index = 0\n",
    "for path in cut_encodings:\n",
    "    dict_ind[path] = []\n",
    "    for i in range(len(cut_encodings[path])):\n",
    "        \n",
    "        dict_ind[path].append(index)\n",
    "        index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTW\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def dtw_sweep_min(query_seq, search_seq, n_step=3):\n",
    "    \"\"\"\n",
    "    Return the minimum DTW cost as `query_seq` is swept across `search_seq`.\n",
    "\n",
    "    Step size can be specified with `n_step`.\n",
    "    \"\"\"\n",
    "\n",
    "    from cython_dtw import _dtw\n",
    "    dtw_cost_func = _dtw.multivariate_dtw_cost_cosine\n",
    "\n",
    "    i_start = 0\n",
    "    n_query = query_seq.shape[0]\n",
    "    n_search = search_seq.shape[0]\n",
    "    min_cost = np.inf\n",
    "\n",
    "    while i_start <= n_search - n_query or i_start == 0:\n",
    "        cost = dtw_cost_func(\n",
    "            query_seq, search_seq[i_start:i_start + n_query], True\n",
    "        )\n",
    "        i_start += n_step\n",
    "        if cost < min_cost:\n",
    "            min_cost = cost\n",
    "\n",
    "    return min_cost\n",
    "\n",
    "def dtw(features):\n",
    "    tensor_features = [torch.from_numpy(f) for f in features]\n",
    "    stacked_features = torch.cat(tensor_features, dim=0)\n",
    "    normalized_features = []\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(stacked_features) \n",
    "    normalized_features = []\n",
    "    for feature in tqdm(features, desc=\"Normalizing Features\"):\n",
    "        normalized_features.append(torch.from_numpy(scaler.transform(feature))) \n",
    "    \n",
    "    num_features = len(normalized_features)\n",
    "    norm_distance_mat = np.zeros((num_features, num_features))\n",
    "    normalized_features = [f.cpu().numpy().astype(np.float64) for f in normalized_features]\n",
    "\n",
    "    for i in tqdm(range(num_features), desc=\"Calculating Distances\"):\n",
    "        dists_i = Parallel(n_jobs=8)(\n",
    "            delayed(dtw_sweep_min)(normalized_features[i], normalized_features[j])\n",
    "            for j in range(i + 1, num_features)\n",
    "        )\n",
    "\n",
    "        for j, dist in zip(range(i + 1, num_features), dists_i):\n",
    "            norm_distance_mat[i, j] = dist\n",
    "            norm_distance_mat[j, i] = dist  \n",
    "            \n",
    "    return norm_distance_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing Features: 100%|██████████| 196/196 [00:00<00:00, 10086.05it/s]\n",
      "Calculating Distances: 100%|██████████| 196/196 [00:09<00:00, 21.52it/s]\n"
     ]
    }
   ],
   "source": [
    "features = [np.ascontiguousarray(f) for f in features]\n",
    "dist_mat_dtw = dtw(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(f\"output/mat/{model_name}/{layer}/{sample_size}\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(out_dir/\"dist_mat_dtw.npy\", dist_mat_dtw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.49587266 0.47204405 0.48535853 0.47438743 0.48968766\n",
      "  0.54713305 0.50010087 0.50088017 0.46095124 0.51188799 0.49713376\n",
      "  0.48815604 0.48472033 0.49642197]\n",
      " [0.49587266 0.         0.5839546  0.4720256  0.51044012 0.52270075\n",
      "  0.62548731 0.56641219 0.49307116 0.52615631 0.65108682 0.49830837\n",
      "  0.45812397 0.44962403 0.5188301 ]\n",
      " [0.47204405 0.5839546  0.         0.44061802 0.45155809 0.48963135\n",
      "  0.55182942 0.50216268 0.50809691 0.47331131 0.58101971 0.48864502\n",
      "  0.4724634  0.48030336 0.48877655]\n",
      " [0.48535853 0.4720256  0.44061802 0.         0.53847369 0.52811584\n",
      "  0.67270445 0.6284018  0.49708372 0.58343753 0.73791956 0.48648047\n",
      "  0.47712191 0.47611693 0.49494202]\n",
      " [0.47438743 0.51044012 0.45155809 0.53847369 0.         0.49879869\n",
      "  0.69301946 0.51725299 0.50324683 0.51323573 0.61831694 0.4591653\n",
      "  0.47655937 0.45815615 0.47005152]]\n",
      "(196, 196)\n"
     ]
    }
   ],
   "source": [
    "print(dist_mat_dtw[0:5, 0:15])\n",
    "print(dist_mat_dtw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering algorithm \n",
    "\n",
    "def cluster(dist_mat, distance_threshold):\n",
    "    num_nodes = dist_mat.shape[0]\n",
    "    graph = {i: set() for i in range(num_nodes)}\n",
    "\n",
    "    for i in range(num_nodes - 1): \n",
    "        for j in range(i + 1, num_nodes):  \n",
    "            if dist_mat[i, j] < distance_threshold:\n",
    "                graph[i].add(j)\n",
    "                graph[j].add(i)  \n",
    "\n",
    "\n",
    "    clusters = []\n",
    "    visited = set()\n",
    "\n",
    "    def bfs(start_node):\n",
    "        \"\"\" Traverse a cluster using BFS \"\"\"\n",
    "        queue = [start_node]\n",
    "        cluster = []\n",
    "        \n",
    "        while queue:\n",
    "            node = queue.pop(0)\n",
    "            if node in visited:\n",
    "                continue \n",
    "            visited.add(node)\n",
    "            cluster.append(node)\n",
    "            queue.extend(graph[node])  \n",
    "\n",
    "        return cluster\n",
    "\n",
    "    for node in range(num_nodes):\n",
    "        if node not in visited:\n",
    "            new_cluster = bfs(node)\n",
    "            clusters.append(new_cluster)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True words Dictionary\n",
    "\n",
    "def parse_text_to_dict(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    data_dict = {}\n",
    "    current_id = None\n",
    "    word_dict = {}\n",
    "\n",
    "    for line in lines: \n",
    "        line = line.strip()\n",
    "\n",
    "        if not line: \n",
    "            continue\n",
    "        \n",
    "        if line.endswith(\":\") and not line.split(\":\")[0].isdigit():\n",
    "            if current_id is not None:\n",
    "                data_dict[current_id] = word_dict\n",
    "            \n",
    "            current_id = line[:-1]\n",
    "            word_dict = {}\n",
    "        else:\n",
    "            parts = line.split(\": \")\n",
    "            if len(parts) == 2:\n",
    "                index, word = parts\n",
    "                word_dict[int(index)] = word.strip()\n",
    "            else:\n",
    "                parts = parts[0].split(\":\")\n",
    "                index = parts[0]\n",
    "                word_dict[int(index)] = \" \"\n",
    "            \n",
    "            if current_id is not None:\n",
    "                data_dict[current_id] = word_dict\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "true_words_dict = parse_text_to_dict(\"data/words_and_indices.txt\")\n",
    "\n",
    "path_dict = {}\n",
    "for path in dict_ind:\n",
    "    path_dict[dict_ind[path][0]] = ''\n",
    "    for i in range(1, len(dict_ind[path])):\n",
    "        path_dict[dict_ind[path][i]] = true_words_dict[path][i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster and WordUnit classes\n",
    "from collections import defaultdict\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self,id, word_dict=None, true_words=None):\n",
    "        self.id = id\n",
    "        self.length = len(word_dict) if word_dict else 0\n",
    "        self.word_dict = word_dict if word_dict is not None else []\n",
    "        self.true_word_dict = true_words if true_words is not None else []\n",
    "    \n",
    "    def add_word_unit(self, id, index, file):\n",
    "        word_unit = WordUnit(file, index, id)\n",
    "        self.length += 1\n",
    "        self.word_dict.append(word_unit)\n",
    "\n",
    "    def add_true_word(self, word):\n",
    "        self.true_word_dict.append(word)\n",
    "\n",
    "    @classmethod\n",
    "    def print_cluster(self, cluster):\n",
    "        print(f\"Cluster {cluster.id}\")\n",
    "        for word in cluster.word_dict:\n",
    "            print(f\"Word {word.id}: Index {word.index} in File {word.file}\")\n",
    "    \n",
    "    def cluster_purity(self):\n",
    "\n",
    "        word_counts = {}\n",
    "        for word in self.true_word_dict:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "        max_count = max(word_counts.values()) if word_counts else 0\n",
    "        cluster_purity = max_count / self.length if self.length > 0 else 0\n",
    "\n",
    "        self.purity = cluster_purity\n",
    "\n",
    "    @classmethod\n",
    "    def duplicate_clusters(self, clusters):\n",
    "        cluster_dict = defaultdict(int)\n",
    "\n",
    "        for cluster in clusters:\n",
    "            cluster_set = frozenset(cluster)  \n",
    "            cluster_dict[cluster_set] += 1  \n",
    "\n",
    "        duplicate_count = sum(1 for count in cluster_dict.values() if count > 1)\n",
    "\n",
    "        return duplicate_count\n",
    "\n",
    "class WordUnit:\n",
    "    def __init__(self, file, index, id):\n",
    "        self.index = int(index)\n",
    "        self.file = file\n",
    "        self.id = int(id)\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "\n",
    "    def add_word_boundaries(self, start_time, end_time):\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0\n",
      "['it', 'it', 'it', 'it', 'it', 'at']\n",
      "\n",
      "Cluster 1\n",
      "['was', 'was']\n",
      "\n",
      "Cluster 2\n",
      "['at', 'asked']\n",
      "\n",
      "Cluster 3\n",
      "['about', 'about']\n",
      "\n",
      "Cluster 4\n",
      "['this', 'this']\n",
      "\n",
      "Cluster 6\n",
      "['in', 'in', 'taken']\n",
      "\n",
      "Cluster 7\n",
      "['their', 'their', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "\n",
      "Cluster 9\n",
      "['that', 'that', 'that', 'that']\n",
      "\n",
      "Cluster 11\n",
      "['became', 'came']\n",
      "\n",
      "Cluster 15\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
      "\n",
      "Cluster 21\n",
      "['as', 'as', 'is', 'is', 'is']\n",
      "\n",
      "Cluster 22\n",
      "['he', 'he', 'he', 'she']\n",
      "\n",
      "Cluster 34\n",
      "['not', 'not']\n",
      "\n",
      "Cluster 37\n",
      "['but', 'and']\n",
      "\n",
      "Cluster 39\n",
      "['kept', 'captain']\n",
      "\n",
      "Cluster 44\n",
      "[' ', ' ']\n",
      "\n",
      "Cluster 50\n",
      "[' ', ' ']\n",
      "\n",
      "Cluster 51\n",
      "['over', 'over']\n",
      "\n",
      "Cluster 54\n",
      "['a', 'appearance', 'a', 'a', 'upon']\n",
      "\n",
      "Cluster 69\n",
      "['had', 'had', 'had']\n",
      "\n",
      "Cluster 74\n",
      "[' ', 'men']\n",
      "\n",
      "Cluster 76\n",
      "['for', 'for']\n",
      "\n",
      "Cluster 85\n",
      "['them', 'them']\n",
      "\n",
      "Cluster 89\n",
      "['home', 'home']\n",
      "\n",
      "Cluster 106\n",
      "['while', 'which']\n",
      "\n",
      "Cluster 108\n",
      "['were', 'were']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clusters_dtw = cluster(dist_mat_dtw, 0.3)\n",
    "clusters = []\n",
    "for i, clust in enumerate(clusters_dtw):\n",
    "    new_cluster = Cluster(id=i)\n",
    "    for w in range(len(clust)):\n",
    "        filename_parts = filenames[clust[w]].split(\"_\")\n",
    "        filename = filename_parts[0]\n",
    "        word_index = int(filename_parts[1])                     \n",
    "        new_cluster.add_word_unit(w, word_index, filename)\n",
    "    clusters.append(new_cluster)\n",
    "\n",
    "for c in clusters:\n",
    "    for word_unit in c.word_dict:\n",
    "        word = true_words_dict[word_unit.file][word_unit.index]\n",
    "        c.add_true_word(word)\n",
    "    \n",
    "    if len(c.word_dict) > 1:  \n",
    "        print(f\"Cluster {c.id}\")                \n",
    "        print(c.true_word_dict)\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
